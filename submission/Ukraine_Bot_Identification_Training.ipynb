{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Install some necessary libraries"
   ],
   "metadata": {
    "id": "F2qd9I7bVZ5Z"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0rLMXrJ7QG5"
   },
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rQQlmMxL7BM1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import *\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# ðŸ’¾ Here we load in the preprocessed meta dataset with features created and explained in Florians Notebook[insert link]. "
   ],
   "metadata": {
    "id": "fYL9dVl4WLlC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsMKPKCWix3J"
   },
   "outputs": [],
   "source": [
    "X_meta = pd.read_csv(\"/content/drive/MyDrive/datasocio/twibot_ukraine_meta.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## lets look inside: ðŸ‘€"
   ],
   "metadata": {
    "id": "S0VDMZQZWf6w"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "OpmMyTT36V92",
    "outputId": "fd030277-7f09-4dbc-beea-9d3480c5ad7a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-9c99e889-6db8-48f7-8fa0-d542ff17233e\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>profile_followers_count</th>\n",
       "      <th>profile_friends_count</th>\n",
       "      <th>is_mt</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>text_sat</th>\n",
       "      <th>has_ellipsis</th>\n",
       "      <th>unk_chars_ratio</th>\n",
       "      <th>cashtags_count</th>\n",
       "      <th>hashtags_count</th>\n",
       "      <th>links_count</th>\n",
       "      <th>mentions_count</th>\n",
       "      <th>emojis_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17461978</td>\n",
       "      <td>15349596</td>\n",
       "      <td>692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17461978</td>\n",
       "      <td>15349596</td>\n",
       "      <td>692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.658333</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17461978</td>\n",
       "      <td>15349596</td>\n",
       "      <td>692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.266667</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17461978</td>\n",
       "      <td>15349596</td>\n",
       "      <td>692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.695833</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17461978</td>\n",
       "      <td>15349596</td>\n",
       "      <td>692</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9c99e889-6db8-48f7-8fa0-d542ff17233e')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-9c99e889-6db8-48f7-8fa0-d542ff17233e button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-9c99e889-6db8-48f7-8fa0-d542ff17233e');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "         ID  profile_followers_count  profile_friends_count  is_mt  is_reply  \\\n",
       "0  17461978                 15349596                    692      0         0   \n",
       "1  17461978                 15349596                    692      0         0   \n",
       "2  17461978                 15349596                    692      0         0   \n",
       "3  17461978                 15349596                    692      0         0   \n",
       "4  17461978                 15349596                    692      0         0   \n",
       "\n",
       "   text_sat  has_ellipsis  unk_chars_ratio  cashtags_count  hashtags_count  \\\n",
       "0  0.583333          True              1.0               0               0   \n",
       "1  0.658333         False              1.0               0               0   \n",
       "2  1.266667         False              1.0               0               1   \n",
       "3  0.695833         False              1.0               0               2   \n",
       "4  0.075000         False              1.0               0               1   \n",
       "\n",
       "   links_count  mentions_count  emojis_count  \n",
       "0            0               0             2  \n",
       "1            2               1             0  \n",
       "2            2               1             0  \n",
       "3            2               1             0  \n",
       "4            0               0             0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸš§ We've found that the models can get quite biased and overfit on the followers and following data. \n",
    "\n",
    "We've found several solution for this. \n",
    "1. we added random noise and randomly added some followers here and there\n",
    "2. we just drop it, since the predictions on pure text data and data with followers were very similar there might be little useful information here but a lot of risk of overfitting.\n"
   ],
   "metadata": {
    "id": "SUIqDKKtWxpr"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kee2kdiyLBS8"
   },
   "outputs": [],
   "source": [
    "X_meta.drop(['profile_followers_count', 'profile_friends_count'],axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHNYwlnKNRZP"
   },
   "outputs": [],
   "source": [
    "ids = X_meta.pop(\"ID\") #we don't need it and don't want it, its a unique identifier => overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## âš™ï¸ We set our configuration data here\n",
    "Most of these are self explanatory, start_lr and min_lr are the learning rates to be used in the scheduler. \n"
   ],
   "metadata": {
    "id": "d_wkMQ6AXPxz"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5BTOdmkR9fy4"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "start_lr = 5e-5\n",
    "min_lr = 1e-8\n",
    "epochs = 10\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ¤– Here we specify our transformer to be used, distilbert is a smaller version of Bert but keeps most of the performance [insert link]"
   ],
   "metadata": {
    "id": "iPMhCOugYP80"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MGehsqnb_dAo"
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MFNdyMkg_iNx"
   },
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_ckpt) \n",
    "backbone = TFDistilBertModel.from_pretrained(model_ckpt, config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdCo5e6T_jWa"
   },
   "outputs": [],
   "source": [
    "backbone.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ“œ These are the preprocessed and tokenized texts and targets that we load in.\n",
    "The function converts these into a tf.data dataset to be consumed by our model.\n"
   ],
   "metadata": {
    "id": "pckP0NcaZHs8"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1tGztop0doc"
   },
   "outputs": [],
   "source": [
    "input_ids = np.load('/content/drive/MyDrive/datasocio/input_ids_twi_uk.npy', allow_pickle = True)\n",
    "attention_masks= np.load('/content/drive/MyDrive/datasocio/attention_masks_twi_uk.npy', allow_pickle = True)\n",
    "target = np.load('/content/drive/MyDrive/datasocio/target_twi_uk.npy',allow_pickle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XyNB-L5k_hHY"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "# Note that some tokenizers also returns 'token_id'. Modify this function accordingly. \n",
    "@tf.function\n",
    "def parse_data(inputs, target):\n",
    "    inputs_ids = inputs['input_ids']\n",
    "    attention_mask = inputs['attention']\n",
    "    meta = inputs['meta']\n",
    "    target = tf.cast(target, tf.int32)\n",
    "    \n",
    "    return {'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'meta': meta}, target"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ”¥Here we define our model it is very simple above the transformer.\n",
    "We feed the text data (tokens) into our backbone model and separately process the meta data to be concatenated."
   ],
   "metadata": {
    "id": "5E_6jtrlZ61I"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZnSwzUrc_kjQ"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    input_ids = tf.keras.Input(shape=(128,),dtype='int32', name = 'input_ids')\n",
    "    attention_masks = tf.keras.Input(shape=(128,),dtype='int32', name = 'attention')\n",
    "    meta = tf.keras.Input(shape=(X_meta.shape[1],),dtype='int32', name = 'meta') \n",
    "    \n",
    "    output = backbone(input_ids,attention_masks)[0]\n",
    "    \n",
    "    output = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(output)\n",
    "    output = tf.keras.layers.GlobalMaxPool1D()(output)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(16,activation='relu')(output)\n",
    "    output = tf.keras.layers.Dropout(0.2)(output)\n",
    "    meta_output = tf.keras.layers.Dense(16,activation='relu')(meta)\n",
    "    concat = tf.keras.layers.concatenate([meta_output, output], name = \"concat\")\n",
    "    \n",
    "    output = tf.keras.layers.Dense(2,activation='softmax', name = \"head\")(concat)\n",
    "    model = tf.keras.models.Model(inputs = [input_ids,attention_masks, meta],outputs = output)\n",
    "    \n",
    "    \n",
    "    for layer in model.layers[:3]:\n",
    "        print(layer)\n",
    "        layer.trainable = False\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gG-widQ_lVB"
   },
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Below we do a standard split for the validation data and we normalize the meta data."
   ],
   "metadata": {
    "id": "CTU3dXNt0fES"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ahvHi0MV_m4h",
    "outputId": "d6305031-080b-41f6-8ce2-7f010bfcf7b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13320"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "followers = X_meta.to_numpy().astype('float32')\n",
    "X_train, X_test, y_train, y_test,train_mask,test_mask, meta_train, meta_test =train_test_split(input_ids,\n",
    "                                                                       target,\n",
    "                                                                       attention_masks,\n",
    "                                                                       followers,\n",
    "                                                                       random_state = seed,\n",
    "                                                                        test_size=0.3)\n",
    "scaler = StandardScaler()\n",
    "meta_train = scaler.fit_transform(meta_train)\n",
    "meta_test = scaler.transform(meta_test)\n",
    "#just some cleaning up below for memory\n",
    "del input_ids, attention_masks, X_meta\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ðŸ¥„ This creates dataloaders for our train and validation data to be more efficiently fed into the model."
   ],
   "metadata": {
    "id": "FP7ADyZK04jy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wOgOzrxG_qty"
   },
   "outputs": [],
   "source": [
    "trainloader = tf.data.Dataset.from_tensor_slices(({'input_ids':X_train, 'attention':train_mask, 'meta': meta_train}, y_train))\n",
    "testloader = tf.data.Dataset.from_tensor_slices(({'input_ids':X_test, 'attention':test_mask, 'meta': meta_test}, y_test))\n",
    "trainloader = (\n",
    "        trainloader\n",
    "        .cache()\n",
    "        .shuffle(4096)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(AUTOTUNE)\n",
    "        \n",
    "    )\n",
    "\n",
    "testloader = (\n",
    "        testloader\n",
    "        .batch(512)\n",
    "        .prefetch(AUTOTUNE)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Below we set up the simplest (but often effective) schedule Reduce Learning Rate on Plateau. \n",
    "When the model stagnates for \"patience\" number of epochs, it multiplies the learning rate by a factor \"factor\" up to min_lr.\n",
    "Additionally we set up early stopping, our free lunch and checkpointing for safety.\n",
    "\n",
    "This was used in the pretraining, for pseudo labelling we do it manually."
   ],
   "metadata": {
    "id": "fkbGqZss11dh"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3RopmHL_re5",
    "outputId": "2f018d05-b485-4160-a39a-f8615a300c16"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "rlr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr= min_lr)\n",
    "es = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n",
    "opt = tf.keras.optimizers.Adam(lr = start_lr)\n",
    "checkpoint_filepath = '/content/drive/MyDrive/datasocio/no_followers_checkpoint_ukraine/'\n",
    "ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    save_freq = 'epoch',\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# We compile and maybe load in weights from previous checkpoints (if available)"
   ],
   "metadata": {
    "id": "Tt8UEIBh2XSV"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k4y4u7hD_svZ"
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer= opt,metrics=['categorical_accuracy', tf.keras.metrics.AUC()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y7RoCzJrODYH"
   },
   "outputs": [],
   "source": [
    "model.load_weights('/content/drive/MyDrive/datasocio/no_followerspseudo.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below we set an array of dates, each date corresponds to a subset of the ukrain data with month march day = date. Note 2627 is actually 26-28th.\n",
    "\n",
    "In the pseudo labelling below, we will randomly select one of these subsets from the ukraine tweets and run a prediction, then a backwards pass on those predictions.\n",
    "\n",
    "You can think of it as bootstrapping, officially this is part of self-supervised learning.\n",
    "\n",
    "This is one of the crudest ways to do it, but for our data absolutely enough.\n",
    "Most commonly one does this every step (batch), and slowly increases the effect of the loss from the pseudo-predicted dataset.\n",
    "\n",
    "We could do this here too, but the subsets are small enough that our reasoning was this suffices.\n",
    "\n",
    "The model was also already pre-trained a few epochs on only the twibot data."
   ],
   "metadata": {
    "id": "EyBEYeP12eG0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TRKunJLS_3OE"
   },
   "outputs": [],
   "source": [
    "dates = [\"_420_\", \"_may_\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "AulUU97L-X7B"
   },
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "  print(f\"Epoch: {epoch}\")\n",
    "  for i in range(10):\n",
    "    #get a subset next 4096 samples from labelled trainign data\n",
    "    subset = trainloader.take(4096)\n",
    "    # pick a subset of unlabelled data => day of ukraine tweets\n",
    "    date = np.random.choice(dates)\n",
    "    #prepare ukraine subset for prediction\n",
    "    X_meta = pd.read_parquet(f\"/content/drive/MyDrive/datasocio/data/ukraine_meta_mar{date}.pq\")\n",
    "    X_meta = X_meta.drop(['followers','following'], axis=1)\n",
    "    input_ids=np.load(f\"/content/drive/MyDrive/datasocio/data/input_ids_mar{date}.npy\")\n",
    "    attention_masks=np.load(f\"/content/drive/MyDrive/datasocio/data/attention_masks_mar{date}.npy\")\n",
    "    meta = X_meta.to_numpy().astype(np.float32)\n",
    "    meta = scaler.transform(meta)\n",
    "    #predict\n",
    "    pseudo = model.predict([input_ids, attention_masks, meta], batch_size = batch_size)\n",
    "    #filer the predictions by those which model is rather sure about => more likely to be correct\n",
    "    mask = (pseudo[:, 0] > 0.9) | (pseudo[:, 1] > 0.9)\n",
    "    #fit on the filtered pseudo labels\n",
    "    model.fit([input_ids[mask], attention_masks[mask], meta[mask]], y = np.around(pseudo[mask]), batch_size = 32)\n",
    "    #fit on our trainign data\n",
    "    model.fit(subset)\n",
    "    #evaluate on labeled data every epoch\n",
    "  model.evaluate(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aMR0jDbQF7YY"
   },
   "outputs": [],
   "source": [
    "#leakage in metadata, to be fixed\n",
    "history=model.fit(trainloader,\n",
    "                        validation_data = testloader,\n",
    "                        batch_size=batch_size,\n",
    "                        callbacks = [rlr, es, ckpt],\n",
    "                        epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K95519CMEazO"
   },
   "outputs": [],
   "source": [
    "model.save_weights('/content/drive/MyDrive/datasocio/no_followerspseudo.h5')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Ukraine Bot Identification Training",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}